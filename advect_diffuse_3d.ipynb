{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f3278e3",
   "metadata": {},
   "source": [
    "### advect_diffuse_3d\n",
    "\n",
    "Use the down sample version of .npz MRI dataset process from `mri_dateset_generator.ipynb`,\n",
    "all config are in yaml file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf67ec6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First choose a config file to apply\n",
    "config_path = \"configs/adpinn_full_runtest.yaml\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707e4cbe",
   "metadata": {},
   "source": [
    "#### 1. Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92874a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "\n",
    "from modules.ad_net import AD_Net\n",
    "from modules.data_module import CharacteristicDomain, DCEMRIDataModule, VelocityDataModule\n",
    "from modules.filtered_modules import create_outlier_filter_mask, FilteredDCEMRIDataModule\n",
    "from utils.config_loader import Train_Config\n",
    "from utils.train_wrapper import train_all_phases\n",
    "from utils.visualize import draw_nifti_slices_with_time, draw_nifti_slices_with_threshold, interactive_quiver\n",
    "from utils.io import load_dcemri_data, save_velocity_mat, load_DTI\n",
    "%matplotlib widget\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# load cfg from yaml path.\n",
    "cfg = Train_Config(config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891b0a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data,mask,pixdim,x,y,z,t = load_dcemri_data(cfg.dcemrinp_data_path)\n",
    "\n",
    "char_domain = CharacteristicDomain(data.shape, mask, t, pixdim, cfg.reload_dataloaders_every_n_epochs, cfg.device)\n",
    "\n",
    "print(\"L_star: \",char_domain.L_star, \"T_star: \", char_domain.T_star)\n",
    "# batch_size is sum over data's point\n",
    "c_dataset = DCEMRIDataModule(data, char_domain,  \n",
    "                           batch_size=int(mask.sum()*len(t)), num_workers=cfg.dataset_num_workers, device=cfg.device)\n",
    "\n",
    "# set up to get num_train_points\n",
    "c_dataset.setup()\n",
    "print(\"num_train_points: \", c_dataset.num_train_points, \"batch_size: \", c_dataset.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fb5765",
   "metadata": {},
   "outputs": [],
   "source": [
    "if cfg.use_DTI:\n",
    "    DTI_tensor,DTI_MD = load_DTI(char_domain, cfg.dti_data_path, data.shape[:3])\n",
    "else:\n",
    "    DTI_tensor = None\n",
    "\n",
    "char_domain.set_DTI_or_coef(DTI_tensor if cfg.use_DTI else 2.4e-4)\n",
    "print(\"Pe_g: \", char_domain.Pe_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622bf29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if cfg.use_DTI:\n",
    "    draw_nifti_slices_with_threshold(DTI_MD, brain_mask=mask, slice_along_axis='y')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a81e2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use front-tracking to get initial velocity field.\n",
    "from utils.velocity_guess import front_tracking_velocity\n",
    "# shape (nx, ny, nz, 3), only use half of the timestep to training, consistant with c_dataset\n",
    "\n",
    "initial_velocity_field = front_tracking_velocity(data[:,:,:,::2], \n",
    "                                                 dt=t[2] - t[0])\n",
    "# scaling, front tracking give unit in cell/min, we need to scale to characteristic velocity\n",
    "initial_velocity_field *= (char_domain.pixdim / char_domain.V_star)\n",
    "\n",
    "# draw initial velocity magnitude\n",
    "vel_mag = np.linalg.norm(initial_velocity_field, axis=-1)\n",
    "# print(pixdim, char_domain.domain_shape, char_domain.L_star, char_domain.T_star, char_domain.V_star)\n",
    "print(\"velocity max and min: \", vel_mag.max(), vel_mag.min())\n",
    "draw_nifti_slices_with_threshold(vel_mag)\n",
    "\n",
    "v_dataset = VelocityDataModule(initial_velocity_field, char_domain,\n",
    "                              batch_size=int(mask.sum()), num_workers=cfg.dataset_num_workers, device=cfg.device)\n",
    "# set up to get num_train_points\n",
    "v_dataset.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720de574",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_quiver(initial_velocity_field[:,:,:,0], initial_velocity_field[:,:,:,1], \n",
    "                   initial_velocity_field[:,:,:,2], char_domain.pixdim,default_elev=-62.76, default_azim=-10.87)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0d580f",
   "metadata": {},
   "source": [
    "#### 2. Network definition\n",
    "\n",
    "Here only define a base advect-diffuse PINN that learn the whole, it is reported in [J. D. Toscano et al. 2025](https://doi.org/10.1101/2025.07.30.667741) , that pure AD instead of Darcy's law cannot capture bimodal velocity distribution. \n",
    "\n",
    "This version has two variants:\n",
    "1. simply using FNN to produce velocity field, without any pruning on solution space.\n",
    "2. physics constrained divergence-free to the velocity net, so raw v_net predict the vector potential $\\Phi(x,y,z)$ instead \n",
    "\n",
    "Also define training sequence, different variant network has different training loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4303f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training variant 1: init C_Net\n",
    "from modules.c_trainer import CNet_Init, CNet_DenoiseInit\n",
    "# Training varient 2: init V_Net (need heuristic estimate)\n",
    "# 3. only optimize v + D, using advect-diffuse.\n",
    "# 4. joint optimize c + v + D, using advect-diffuse\n",
    "from modules.ad_trainer import ADPINN_InitV, ADPINN_PDE_V, ADPINN_Joint\n",
    "\n",
    "# write a trainner getter to factorize.\n",
    "\n",
    "def ADPINN_trainer_getter(train_phase, ad_net, phase_cfg={}):\n",
    "    global c_dataset\n",
    "    if train_phase == CNet_Init.train_phase:\n",
    "        pinn_model = CNet_Init(ad_net.c_net, c_dataset.num_train_points)\n",
    "        datamodule = c_dataset\n",
    "    elif train_phase == CNet_DenoiseInit.train_phase:\n",
    "        pinn_model = CNet_DenoiseInit(ad_net.c_net, c_dataset.num_train_points)\n",
    "        datamodule = c_dataset\n",
    "    elif train_phase == ADPINN_InitV.train_phase:\n",
    "        pinn_model = ADPINN_InitV(ad_net, v_dataset.num_train_points)\n",
    "        datamodule = v_dataset # just coarse initialie, do not need rbar.\n",
    "    elif train_phase == \"filter\":\n",
    "        batch_size = phase_cfg.get(\"batch_size\", 200_000)\n",
    "        # filter means filter on c_dataset.\n",
    "        ad_net.c_net.to(cfg.device)\n",
    "        valid_mask = create_outlier_filter_mask(ad_net.c_net, c_dataset.X_train, batch_size)\n",
    "        c_dataset = FilteredDCEMRIDataModule(c_dataset, valid_mask)\n",
    "        c_dataset.setup() # setup to get actual train_num_points\n",
    "        pinn_model = None\n",
    "        datamodule = c_dataset\n",
    "    elif train_phase == ADPINN_PDE_V.train_phase:\n",
    "        pinn_model = ADPINN_PDE_V(ad_net, c_dataset.num_train_points)\n",
    "        datamodule = c_dataset\n",
    "    elif train_phase.startswith(ADPINN_Joint.train_phase):\n",
    "        pinn_model = ADPINN_Joint(ad_net, c_dataset.num_train_points)\n",
    "        datamodule = c_dataset\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown train_phase {train_phase}\")\n",
    "    return pinn_model, datamodule\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b303ed",
   "metadata": {},
   "source": [
    "#### 3. Training step\n",
    "\n",
    "- first we has initializaiton for c and v net\n",
    "- then pde loss for only v net\n",
    "- finally joint optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16edc20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model and trainer or load from checkpoint:\n",
    "ad_net = AD_Net(c_layers=[4] + [cfg.c_neuron_num for _ in range(cfg.hid_layer_num)] + [1],\n",
    "    u_layers=[3] + [cfg.neuron_num for _ in range(cfg.hid_layer_num)] + [3],\n",
    "    data=data,C_star=c_dataset.C_star,\n",
    "    incompressible=False,\n",
    "    char_domain=char_domain,\n",
    "    positional_encoding=cfg.use_positional_encoding,\n",
    "    freq_nums=cfg.positional_encode_nums,\n",
    "    gamma_space=cfg.position_encode_freq_scale,\n",
    "    use_learnable_D=cfg.use_learnable_D)\n",
    "\n",
    "if cfg.do_training:\n",
    "    pinn_model = train_all_phases(ad_net, ADPINN_trainer_getter, cfg)\n",
    "else:\n",
    "    checkpoint = torch.load(cfg.ckpt_path, map_location=\"cpu\")\n",
    "    if \"state_dict\" in checkpoint:\n",
    "        pinn_model = ADPINN_trainer_getter(checkpoint.get(\"train_phase\"), ad_net)[0]\n",
    "        pinn_model.load_state_dict(checkpoint['state_dict'], strict=True)\n",
    "    else:\n",
    "        pinn_model = ADPINN_Joint(ad_net, c_dataset.num_train_points)\n",
    "        ad_net.load_state_dict(checkpoint['state_dict'], strict=False)\n",
    "\n",
    "# After training, visualize the learned velocity field and diffusivity\n",
    "# Extract learned parameters\n",
    "D_learned = pinn_model.ad_net.D.item()\n",
    "\n",
    "print(f\"Learned diffusivity D: {D_learned}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a062a7d7",
   "metadata": {},
   "source": [
    "#### 4. Visualize result.\n",
    "Finally we will save the velocity mat to run GLAD (get pathline figure) in matlab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1104b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. Extract learned velocity field on a grid for visualization\n",
    "pinn_model.to(cfg.device)\n",
    "# using mask to filter out the background vectors\n",
    "# _ = interactive_quiver(vx, vy, vz, pixdim, default_elev=-62.76, default_azim=-10.87)\n",
    "nx, ny, nz = data.shape[0], data.shape[1], data.shape[2]\n",
    "v_fig, vx,vy,vz = pinn_model.v_net.draw_velocity_volume()\n",
    "print(v_fig.shape)\n",
    "# change vx,vy,vz from characteristic to mm/min then cell/min\n",
    "vx = vx * (char_domain.V_star[0] / char_domain.pixdim[0])\n",
    "vy = vy * (char_domain.V_star[1] / char_domain.pixdim[1])\n",
    "vz = vz * (char_domain.V_star[2] / char_domain.pixdim[2])\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.imshow(v_fig)\n",
    "# use a grid (from real to characteristic) to extract velocity field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5835d99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use pinn_model.val_slice_4d to draw concentration slices\n",
    "c_vis_list = pinn_model.c_net.draw_concentration_slices()\n",
    "plt.figure(figsize=(6,3))\n",
    "plt.imshow(c_vis_list, cmap='jet', vmin=0, vmax=1)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cb6f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Extract all density field with x,y,z,t 4D grid for visualization\n",
    "\n",
    "grid_tensor_4d = char_domain.get_characteristic_geotimedomain()  # include time part\n",
    "print(grid_tensor_4d.shape)\n",
    "def predict_concentration_4d(model, pts4, nx, ny, nz, nt, batch_size=200_000):\n",
    "    out_list = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, pts4.shape[0], batch_size):\n",
    "            chunk = pts4[i:i+batch_size]\n",
    "            pred = model.c_net(chunk)\n",
    "            out_list.append(pred.cpu())\n",
    "    C_flat = torch.cat(out_list, dim=0).numpy().reshape(nx, ny, nz, nt)\n",
    "    # C_flat = torch.cat(out_list, dim=0).numpy().reshape(nx, ny, nz, nt)\n",
    "    return C_flat\n",
    "\n",
    "C_pred_4d = predict_concentration_4d(pinn_model, grid_tensor_4d, nx, ny, nz, len(t), batch_size=200_000)\n",
    "\n",
    "# Back to physical units\n",
    "C_pred_4d *= c_dataset.C_star\n",
    "print(\"Pred shape:\", C_pred_4d.shape, \"True shape:\", data.shape)  # (nx, ny, nz, nt)\n",
    "\n",
    "_ = draw_nifti_slices_with_time(C_pred_4d, data, mask)\n",
    "# try calculate loss with true data\n",
    "# from sklearn.metrics import mean_squared_error\n",
    "# mse_overall = mean_squared_error(data.flatten(), C_pred_4d.flatten())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b173fbb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export velocity to .mat format so that we could run GLAD using matlab\n",
    "# the rOMT code require velocity reshaped to N*3 x 1, with unit cell/min\n",
    "# also save D as real unit if not cfg.use_DTI, else save D_normalized as DTI enhanced\n",
    "# to tell matlab how to simulate advect-diffuse equation\n",
    "save_velocity_mat(vx,vy,vz,pixdim, \n",
    "                  D=pinn_model.ad_net.D.item() if not cfg.use_DTI else pinn_model.ad_net.D_normalized.item(), \n",
    "                  use_DTI=cfg.use_DTI\n",
    "                  ,path=f\"{cfg.result_folder}/predict_velocity.mat\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
