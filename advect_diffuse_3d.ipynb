{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f3278e3",
   "metadata": {},
   "source": [
    "### advect_diffuse_3d\n",
    "\n",
    "Use the down sample version of .nii MRI data, hope for a good interpolation. We mostly use DeepXDE to build research-grade PINN, with several optimization module, which is enough for DCE-MRI task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6b28a8",
   "metadata": {},
   "source": [
    "#### 0. parameter adjust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be6b9f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "do_training = True\n",
    "# if do not training, please provide pth path to load.\n",
    "# ckpt_path = \"data/adpinn_pde_v_32_20-52_l8_n128_20250930.pth\"\n",
    "\n",
    "continue_training = False\n",
    "# if continue training, please provide ckpt path to load.\n",
    "ckpt_path = \"tb_logs/seqtrain_ADPINN/version_7/checkpoints/epoch=1280-step=1281.ckpt\"\n",
    "\n",
    "neuron_num = 150 # 150\n",
    "hid_layer_num = 5 # 5\n",
    "\n",
    "positional_encode_nums = (20, 20, 20, 0)  # (x,y,z,t)\n",
    "position_encode_freq_scale = 1.0\n",
    "# DTI will cause anisotropic diffusion (2nd order term of c_net)\n",
    "use_DTI = True\n",
    "\n",
    "# RBAR will compute c_grad and c_laplacian for every point in every train phase,\n",
    "# so adjust batch to be smaller to avoid CUDA-OOM; \n",
    "# also the resampling will be the bottleneck of training speed, so could set reload_dataloaders_every_n_epochs higher.\n",
    "enable_rbar = False\n",
    "reload_dataloaders_every_n_epochs = 250 if enable_rbar else 0\n",
    "\n",
    "dataset_num_workers = 1\n",
    "\n",
    "# define ordered phases explicitly\n",
    "phase_order = [\"init_c\", \"init_c_denoise\", \"init_v\", \"pde_v\" ,\"joint\"]\n",
    "\n",
    "train_setting = {\n",
    "    # \"init_c\":{\n",
    "    #     \"learning_rate\": 1e-4,\n",
    "    #     \"max_epochs\": 4000,\n",
    "    #     \"batch_size\": 8192, # 222_000\n",
    "    # },\n",
    "    # \"init_c_denoise\":{\n",
    "    #     \"learning_rate\": 1e-4,\n",
    "    #     \"max_epochs\": 800,\n",
    "    #     \"batch_size\": 8192, # 222_000\n",
    "    # },\n",
    "    # \"init_v\":{\n",
    "    #     \"learning_rate\": 1e-4,\n",
    "    #     \"max_epochs\": 800,\n",
    "    #     \"batch_size\": 8192, # 222_000\n",
    "    # },\n",
    "    # \"pde_v\":{\n",
    "    #     \"learning_rate\": 1e-4,\n",
    "    #     \"max_epochs\": 20,\n",
    "    #     \"batch_size\": 8192, # 222_000\n",
    "    # },\n",
    "    \"joint\":{\n",
    "        \"learning_rate\": 1e-5,\n",
    "        \"max_epochs\": 1000,\n",
    "        \"batch_size\": 222_000,\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707e4cbe",
   "metadata": {},
   "source": [
    "#### 1. Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b92874a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "\n",
    "import torch.nn as nn\n",
    "import lightning as L\n",
    "from lightning.pytorch import Trainer\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "\n",
    "from modules.ad_net import AD_Net\n",
    "from datetime import datetime\n",
    "from modules.data_module import CharacteristicDomain, DCEMRIDataModule, VelocityDataModule\n",
    "from utils.visualize import interactive_quiver, fixed_quiver_image, draw_nifti_slices_with_time, draw_nifti_slices_with_threshold\n",
    "from utils.io import load_dcemri_data, save_velocity_mat, load_DTI\n",
    "%matplotlib widget\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "891b0a39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_shape:  (32, 40, 32, 18) pixdim:  [0.5 0.5 0.5]\n",
      "domain_shape:  (32,) (40,) (32,) (18,)\n",
      "min_c:  0.0 max_c:  299.53845\n",
      "L_star:  [7.75 9.75 7.75] T_star:  34.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data,mask,pixdim,x,y,z,t = load_dcemri_data(\"data/dataset_downsampled.npz\")\n",
    "\n",
    "# Also need to transform anisotropic diffusivity if needed here.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "char_domain = CharacteristicDomain(data.shape, mask, t, pixdim, device)\n",
    "\n",
    "print(\"L_star: \",char_domain.L_star, \"T_star: \", char_domain.T_star)\n",
    "# batch_size is sum over data's point\n",
    "c_dataset = DCEMRIDataModule(data, char_domain,  \n",
    "                           batch_size=int(mask.sum()*len(t)), num_workers=dataset_num_workers, device=device)\n",
    "\n",
    "# set up to get num_train_points\n",
    "c_dataset.setup()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0fb5765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DTI shape:  (128, 160, 128, 3, 3)\n",
      "Resized DTI shape:  (32, 40, 32, 3, 3)\n",
      "DTI_MD min:  0.004034522169099224 DTI_MD max:  0.07258995996378136 DTI_MD mean:  0.020403731246638808\n",
      "Pe_g:  2.083537581699346\n"
     ]
    }
   ],
   "source": [
    "if use_DTI:\n",
    "    DTI_tensor,DTI_MD = load_DTI(char_domain, \"data/DCE_nii_data/dti_tensor_3_3.mat\", data.shape[:3])\n",
    "else:\n",
    "    DTI_tensor = None\n",
    "\n",
    "char_domain.set_DTI_or_coef(DTI_tensor if use_DTI else 2.4e-4)\n",
    "print(\"Pe_g: \", char_domain.Pe_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "622bf29b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Volume shape: (32, 40, 32) dtype: float64\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3e664e003c448e3933e1ce7645f44cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=16, description='z', max=31), FloatSlider(value=0.5, description='thr', …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if use_DTI:\n",
    "    draw_nifti_slices_with_threshold(DTI_MD, brain_mask=mask)\n",
    "\n",
    "output_tag = (f\"{data.shape[0]}_{int(t[0])}-{int(t[-1])}\"\n",
    "            f\"_pos{position_encode_freq_scale}{positional_encode_nums[0]}\"\n",
    "            f\"_l{hid_layer_num}_n{neuron_num}{'' if DTI_tensor is None else '_DTI'}\"\n",
    "            f\"_{datetime.now().strftime('%m%d')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a81e2fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "velocity max and min:  1.627875979102795 0.0\n",
      "Volume shape: (32, 40, 32) dtype: float64\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "416c17eabe1a41a0af1b805e0d4fe3a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=16, description='z', max=31), FloatSlider(value=0.5, description='thr', …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# use front-tracking to get initial velocity field.\n",
    "from utils.front_tracking import front_tracking_velocity\n",
    "# shape (nx, ny, nz, 3), only use half of the timestep to training, consistant with c_dataset\n",
    "\n",
    "initial_velocity_field = front_tracking_velocity(data[:,:,:,::2], \n",
    "                                                 dt=t[2] - t[0])\n",
    "# scaling using V_star\n",
    "initial_velocity_field /= char_domain.V_star\n",
    "# draw initial velocity magnitude\n",
    "vel_mag = np.linalg.norm(initial_velocity_field, axis=-1)\n",
    "\n",
    "# print(pixdim, char_domain.domain_shape, char_domain.L_star, char_domain.T_star, char_domain.V_star)\n",
    "print(\"velocity max and min: \", vel_mag.max(), vel_mag.min())\n",
    "\n",
    "draw_nifti_slices_with_threshold(vel_mag)\n",
    "\n",
    "v_dataset = VelocityDataModule(initial_velocity_field, char_domain,\n",
    "                              batch_size=int(mask.sum()), num_workers=dataset_num_workers, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0d580f",
   "metadata": {},
   "source": [
    "#### 2. Network definition\n",
    "\n",
    "Here only define a base advect-diffuse PINN that learn the whole, it is reported in [J. D. Toscano et al. 2025](https://doi.org/10.1101/2025.07.30.667741) , that pure AD instead of Darcy's law cannot capture bimodal velocity distribution. \n",
    "\n",
    "This version has two variants:\n",
    "1. simply using FNN to produce velocity field, without any pruning on solution space.\n",
    "2. physics constrained divergence-free to the velocity net, so raw v_net predict the vector potential $\\Phi(x,y,z)$ instead \n",
    "\n",
    "Also define training sequence, different variant network has different training loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b4303f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training variant 1: init C_Net\n",
    "from modules.c_trainer import CNet_Init, CNet_DenoiseInit\n",
    "# Training varient 2: init V_Net (need heuristic estimate)\n",
    "# 3. only optimize v + D, using advect-diffuse.\n",
    "# 4. joint optimize c + v + D, using advect-diffuse\n",
    "from modules.ad_trainer import ADPINN_InitV, ADPINN_PDE_V, ADPINN_Joint\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b303ed",
   "metadata": {},
   "source": [
    "#### 3. Training step\n",
    "\n",
    "- first we has initializaiton for c and v net\n",
    "- then pde loss for only v net\n",
    "- finally joint optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16edc20d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type    | Params | Mode \n",
      "--------------------------------------------\n",
      "0 | ad_net  | AD_Net  | 219 K  | train\n",
      "1 | c_net   | C_Net   | 109 K  | train\n",
      "2 | v_net   | V_Net   | 109 K  | train\n",
      "3 | L2_loss | MSELoss | 0      | train\n",
      "--------------------------------------------\n",
      "219 K     Trainable params\n",
      "0         Non-trainable params\n",
      "219 K     Total params\n",
      "0.877     Total estimated model params size (MB)\n",
      "31        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e50b6a83a8464730b7a9257387f85bed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "w:\\bigTool\\Anaconda\\envs\\AI\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:433: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.\n",
      "w:\\bigTool\\Anaconda\\envs\\AI\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:433: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.\n",
      "w:\\bigTool\\Anaconda\\envs\\AI\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:310: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=5). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da435dcc9fe44956bef26cab805e21db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Instantiate the model and trainer or load from checkpoint:\n",
    "ad_net = AD_Net(c_layers=[4] + [neuron_num for _ in range(hid_layer_num)] + [1],\n",
    "    u_layers=[3] + [neuron_num for _ in range(hid_layer_num)] + [3],\n",
    "    data=data,C_star=c_dataset.C_star,\n",
    "    incompressible=False,\n",
    "    char_domain=char_domain,\n",
    "    freq_nums=positional_encode_nums,\n",
    "    gamma_space=position_encode_freq_scale)\n",
    "\n",
    "if do_training:\n",
    "    logger = TensorBoardLogger(\"tb_logs\", name=\"seqtrain_ADPINN\")\n",
    "    # save every 100 epochs\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        dirpath=\"checkpoints/\",\n",
    "        filename=\"pinn-{epoch:04d}\",\n",
    "        save_top_k=0,                # keep latest checkpoints\n",
    "        every_n_epochs=100            # save every 100 epochs\n",
    "    )\n",
    "\n",
    "    # because large batch pointset, steps in one epoch is small.\n",
    "    trainer = Trainer(reload_dataloaders_every_n_epochs= reload_dataloaders_every_n_epochs, log_every_n_steps=5, \n",
    "                      check_val_every_n_epoch = 100, callbacks=[checkpoint_callback] , logger=logger)\n",
    "    last_model = None\n",
    "\n",
    "    train_phase_togo = [phase for phase in phase_order if phase in train_setting]\n",
    "\n",
    "    if continue_training:\n",
    "        # first check whether the ckpt_path is a lightning checkpoint or state_dict\n",
    "        checkpoint = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "        # also check whether sequential training step meet our next training phase\n",
    "        if \"state_dict\" in checkpoint and \"train_phase\" in checkpoint and checkpoint[\"train_phase\"] == train_phase_togo[0]:\n",
    "            # could directly fit the trainer.\n",
    "            last_epoch = checkpoint[\"epoch\"]\n",
    "            total_epochs = last_epoch\n",
    "            print(f\"Continue training from phase {checkpoint['train_phase']} with epoch {last_epoch}\")\n",
    "        else:\n",
    "            ad_net.load_state_dict(checkpoint['state_dict'], strict=False)\n",
    "            total_epochs = 0\n",
    "            continue_training = False # already loaded state_dict, no need to continue training\n",
    "    else:\n",
    "        total_epochs = 0\n",
    "    \n",
    "    for phase in train_phase_togo:\n",
    "        cfg = train_setting[phase]\n",
    "        lr = cfg.get(\"learning_rate\", 1e-3)\n",
    "        max_epochs = cfg.get(\"max_epochs\", 1000)\n",
    "        batch_size = cfg.get(\"batch_size\", 222_000)\n",
    "        if phase == \"init_c\":\n",
    "            pinn_model = CNet_Init(ad_net.c_net, c_dataset.num_train_points, \n",
    "                                   learning_rate=lr, enable_rbar=enable_rbar)\n",
    "            datamodule = c_dataset\n",
    "            if enable_rbar:\n",
    "                datamodule.set_RBA_resample_model(pinn_model)\n",
    "        elif phase == \"init_c_denoise\":\n",
    "            pinn_model = CNet_DenoiseInit(ad_net.c_net, c_dataset.num_train_points, \n",
    "                                          learning_rate=lr, enable_rbar=enable_rbar)\n",
    "            datamodule = c_dataset\n",
    "            if enable_rbar:\n",
    "                datamodule.set_RBA_resample_model(pinn_model)\n",
    "        elif phase == \"init_v\":\n",
    "            pinn_model = ADPINN_InitV(ad_net, c_dataset.num_train_points, learning_rate=lr)\n",
    "            datamodule = v_dataset # just coarse initialie, do not need rbar.\n",
    "        elif phase == \"pde_v\":\n",
    "            pinn_model = ADPINN_PDE_V(ad_net, c_dataset.num_train_points, learning_rate=lr)\n",
    "            datamodule = c_dataset\n",
    "        elif phase == \"joint\":\n",
    "            pinn_model = ADPINN_Joint(ad_net, c_dataset.num_train_points, learning_rate=lr)\n",
    "            datamodule = c_dataset\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        datamodule.batch_size = batch_size\n",
    "        # update epochs per phase on the same Trainer\n",
    "        total_epochs += max_epochs\n",
    "        trainer.fit_loop.max_epochs = total_epochs\n",
    "\n",
    "        # optional: tag phase in TensorBoard\n",
    "        trainer.logger.experiment.add_text(\"phase/start\", phase, global_step=trainer.global_step)\n",
    "        # pinn_model.log = lambda name, value, **kw: super(type(pinn_model), pinn_model).log(f\"{phase}/{name}\", value, **kw)\n",
    "        trainer.fit(pinn_model, datamodule=datamodule,\n",
    "                    ckpt_path=ckpt_path if continue_training and last_model is None else None)\n",
    "        ckpt_name = (f\"data/adpinn_{phase}_{output_tag}.pth\")\n",
    "        trainer.save_checkpoint(ckpt_name)\n",
    "        \n",
    "        last_model = pinn_model\n",
    "    \n",
    "    pinn_model = last_model\n",
    "else:\n",
    "    if continue_training:\n",
    "        checkpoint = torch.load(ckpt_path)\n",
    "        ad_net.load_state_dict(checkpoint, strict=False)\n",
    "    pinn_model = ADPINN_Joint(ad_net)\n",
    "\n",
    "\n",
    "# After training, visualize the learned velocity field and diffusivity\n",
    "# Extract learned parameters\n",
    "D_learned = pinn_model.ad_net.D.item()\n",
    "\n",
    "print(f\"Learned diffusivity D: {D_learned}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a062a7d7",
   "metadata": {},
   "source": [
    "#### 4. Visualize result.\n",
    "Finally we will save the velocity mat to run GLAD (get pathline figure) in matlab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1104b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. Extract learned velocity field on a grid for visualization\n",
    "pinn_model.to(device)\n",
    "# using mask to filter out the background vectors\n",
    "# _ = interactive_quiver(vx, vy, vz, pixdim, default_elev=-62.76, default_azim=-10.87)\n",
    "nx, ny, nz = data.shape[0], data.shape[1], data.shape[2]\n",
    "v_fig, vx,vy,vz = pinn_model.draw_velocity_volume();\n",
    "print(v_fig.shape)\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.imshow(v_fig)\n",
    "# use a grid (from real to characteristic) to extract velocity field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5835d99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use pinn_model.val_slice_4d to draw concentration slices\n",
    "c_vis_list = pinn_model.c_net.draw_concentration_slices()\n",
    "plt.figure(figsize=(6,3))\n",
    "plt.imshow(c_vis_list, cmap='jet', vmin=0, vmax=1)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cb6f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Extract all density field with x,y,z,t 4D grid for visualization\n",
    "\n",
    "grid_tensor_4d = char_domain.get_characteristic_geotimedomain()  # include time part\n",
    "print(grid_tensor_4d.shape)\n",
    "def predict_concentration_4d(model, pts4, nx, ny, nz, nt, batch_size=200_000):\n",
    "    out_list = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, pts4.shape[0], batch_size):\n",
    "            chunk = pts4[i:i+batch_size]\n",
    "            pred = model.c_net(chunk)\n",
    "            out_list.append(pred.cpu())\n",
    "    C_flat = torch.cat(out_list, dim=0).numpy().reshape(nx, ny, nz, nt)\n",
    "    # C_flat = torch.cat(out_list, dim=0).numpy().reshape(nx, ny, nz, nt)\n",
    "    return C_flat\n",
    "\n",
    "C_pred_4d = predict_concentration_4d(pinn_model, grid_tensor_4d, nx, ny, nz, len(t), batch_size=200_000)\n",
    "\n",
    "# Back to physical units\n",
    "C_pred_4d *= c_dataset.C_star\n",
    "print(\"Pred shape:\", C_pred_4d.shape, \"True shape:\", data.shape)  # (nx, ny, nz, nt)\n",
    "\n",
    "_ = draw_nifti_slices_with_time(C_pred_4d, data, mask)\n",
    "# try calculate loss with true data\n",
    "# from sklearn.metrics import mean_squared_error\n",
    "# mse_overall = mean_squared_error(data.flatten(), C_pred_4d.flatten())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b173fbb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export velocity to .mat format so that we could run GLAD using matlab\n",
    "# the rOMT code require velocity reshaped to N*3 x 1, with unit cell/min\n",
    "save_velocity_mat(vx,vy,vz,pixdim,path=f\"data/ad_net_velocity_{output_tag}.mat\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
